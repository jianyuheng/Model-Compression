# model-compression-and-acceleration

### REFERENCE

- [Acceleration and Model Compression](https://handong1587.github.io/deep_learning/2015/10/09/acceleration-model-compression.html#compressing-deep-neural-network)

### ðŸŒŸ Overview

- __[2018-arXiv] Recent Advances in Efficient Computation of Deep Convolutional Neural Networks__[`paper`](https://arxiv.org/pdf/1802.00939.pdf)
- **[2018-arXiv] A Survey on Acceleration of Deep Convolutional Neural Networks**[`paper`](https://arxiv.org/abs/1802.00939)
- __[2017-arXiv] A Survey of Model Compression and Acceleration for Deep Neural Networks__ [`paper`](https://arxiv.org/abs/1710.09282)

- __[2017-arXiv] Model compression as constrained optimization, with application to neural nets. Part I: general framework__ [`paper`](https://arxiv.org/abs/1707.01209)
- __[2017-arXiv] Model compression as constrained optimization, with application to neural nets. Part II: quantization__[`paper`](https://arxiv.org/abs/1707.04319)

### ðŸŒŸ Compact Network Design

- __[2018-CVPR] IGCV2: Interleaved Structured Sparse Convolutional Neural Networks__ [`paper`](https://www.semanticscholar.org/paper/IGCV2%3A-Interleaved-Structured-Sparse-Convolutional-Xie-Wang/a2afaa782be91f5baf9e9f1794d57dd29143cbf4)
- __[2018-arXiv] SqueezeNext: Hardware-Aware Neural Network Design__ [`paper`](https://www.semanticscholar.org/paper/IGCV2%3A-Interleaved-Structured-Sparse-Convolutional-Xie-Wang/a2afaa782be91f5baf9e9f1794d57dd29143cbf4) [`code`](https://github.com/homles11/IGCV3)
- __[2018-arXiv] IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks__ [`paper`](https://arxiv.org/pdf/1803.10615)
- __[2018-CVPR] MobileNetV2: Inverted Residuals and Linear Bottlenecks__ [`paper`](https://arxiv.org/pdf/1801.04381.pdf)[`code`](https://github.com/MG2033/MobileNet-V2)
- __[2017-CVPR] SENet: Squeeze-and-Excitation Networks__[`paper`](https://arxiv.org/abs/1709.01507) [`code`](https://github.com/hujie-frank/SENet)
- __[2017-CVPR] MobileNetsV1: Efficient Convolutional Neural Networks for Mobile Vision Applications__[`paper`](https://arxiv.org/pdf/1704.04861.pdf) [`code`](https://github.com/Zehaos/MobileNet)
- __[2017-CVPR] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices__[`paper`](https://arxiv.org/pdf/1707.01083.pdf) [`code`](https://github.com/MG2033/ShuffleNet)
- __[2017-CVPR] ResNeXt: Aggregated Residual Transformations for Deep Neural Networks__ [`paper`](https://arxiv.org/pdf/1611.05431.pdf) [`code`](https://github.com/wenxinxu/ResNeXt-in-tensorflow)
- __[2017-CVPR] Xception: Deep Learning with Depthwise Separable Convolutions__ [`paper`](https://arxiv.org/pdf/1611.05431.pdf) [`code`](https://github.com/kwotsin/TensorFlow-Xception)
- __[2017-ICCV] ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression__ [`paper`](https://arxiv.org/abs/1707.06342) [`code`](https://github.com/Roll920/ThiNet)
- __[2017-CVPR] SEP-Nets: Small and Effective Pattern Networks__ [`paper`](https://arxiv.org/pdf/1706.03912.pdf) 

- __[2016-ICLR] SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size__ [`paper`](https://arxiv.org/pdf/1602.07360.pdf) [`code`](https://github.com/DeepScale/SqueezeNet)

### ðŸŒŸ Distillation

- __[2018-ICLR]  Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy__[`paper`](https://arxiv.org/pdf/1803.10615) 

- __[2018-ICLR] Model compression via distillation and quantization__ [`paper`](https://arxiv.org/abs/1503.02531) 
- __[2018-AAAI] DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer__[`paper`](https://arxiv.org/pdf/1707.01220.pdf)
- __[2017-CVPR] Like What You Like: Knowledge Distill via Neuron Selectivity Transfer__[`paper`](https://arxiv.org/abs/1707.01219)
- **[2017-CVPR] A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning**[`paper`](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)

- __[2016-CVPR] Cross Model Distillation for Supervision Transfer__ [`paper`](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gupta_Cross_Modal_Distillation_CVPR_2016_paper.pdf) [`code`](https://github.com/DeepScale/SqueezeNet)
- __[2016-ICLR] Net2net: Accelerating learning via knowledge transfer__[`paper`](https://arxiv.org/abs/1511.05641)

- __[2015-ICLR] FitNets: Hints for Thin Deep Nets__ [`paper`](https://arxiv.org/pdf/1412.6550.pdf) 

### ðŸŒŸ Pruning

- __[2018-ICLR] To prune, or not to prune: exploring the efficacy of pruning for model compression__ [`paper`](https://arxiv.org/abs/1710.01878)Â 
- __[2018-CVPR] NISP: Pruning Networks using Neuron Importance Score Propagation__ [`paper`](https://arxiv.org/pdf/1711.05908.pdf)Â 
- __[2018-CVPR] â€œLearning-Compressionâ€ Algorithms for Neural Net Pruning__ [`paper`](http://faculty.ucmerced.edu/mcarreira-perpinan/papers/cvpr18.pdf)
- __[2018-ICLR]Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers__ [`paper`](https://openreview.net/pdf?id=HJ94fqApW)Â 
- __[2018-ICLR] Efficient Sparse-Winograd Convolutional Neural Networks__  [`paper`](https://openreview.net/pdf?id=r1rqJyHKg)Â 

- **[2017-ICCV] Channel Pruning for Accelerating Very Deep Neural Networks** [`paper`](https://arxiv.org/pdf/1707.06168.pdf) [`code`](https://github.com/yihui-he/channel-pruning)

- __[2018-WACV] Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks__  [`paper`](https://arxiv.org/abs/1801.10447)Â 
- **[2018-ICML] Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions** [`paper`](https://arxiv.org/abs/1806.09228) [`code`](https://github.com/Sandbox3aster/Deep-K-Means-pytorch)

### ðŸŒŸ Binarization

- **[2016-ArXiv] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1** [`paper`](https://arxiv.org/pdf/1602.02830.pdf) [`code`](https://github.com/MatthieuCourbariaux/BinaryNet)
- **[2016-ECCV] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks** [`paper`](https://arxiv.org/pdf/1603.05279.pdf) [`code`](https://github.com/allenai/XNOR-Net)

### ðŸŒŸ Quantization

- __[2018-ICLR]  Variational Network Quantization__[`paper`](https://arxiv.org/pdf/1803.10615.pdf) 
- **[2018-AAAI] Deep Neural Network Compression with Single and Multiple Level Quantization** [`paper`](https://arxiv.org/abs/1803.03289)
- __[2018-CVPR] Google: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference__ [`paper`](https://arxiv.org/pdf/1712.05877.pdf) 
- **[2018-ArXiv] Google: Quantizing deep convolutional networks for efficient inference: A whitepaper**[`paper`](https://arxiv.org/abs/1806.08342)
- __[2018-ICLR] Training and Inference with Integers in Deep Neural Networks__ [`paper`](https://openreview.net/forum?id=HJGXzmspb)
- __[2018-arXiv] On the Universal Approximability of Quantized ReLU Neural Networks__ [`paper`](https://arxiv.org/pdf/1802.03646.pdf) 

### ðŸŒŸ Low Rank Approximation



### ðŸŒŸ Accelerating / Fast Algorithms

- __[2018-AAAI] Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks__[`paper`](https://arxiv.org/abs/1712.07493)
- **[2018-arXiv] Uber SBNet: Sparse Blocks Network for Fast Inference**[`paper`](https://arxiv.org/abs/1801.02108)

