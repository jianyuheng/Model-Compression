# Model-Compression
Technology of compressing model.

## [Introduction](https://cloud.tencent.com/developer/article/1005738)

## çŸ¥ä¹Ž
- [æ·±åº¦å­¦ä¹ æ¨¡åž‹åŽ‹ç¼©è¯»ä¹¦ç¬”è®°](https://zhuanlan.zhihu.com/p/32223484)
- [æ¨¡åž‹åŽ‹ç¼©é‚£äº›äº‹](https://zhuanlan.zhihu.com/p/28439056)

## Papers & Code
- __[2016-ICLR] SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size__ [`paper`](https://arxiv.org/pdf/1602.07360.pdf)
[`code`](https://github.com/DeepScale/SqueezeNet)

- __[2016-ICLR] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding__ [`paper`](https://arxiv.org/pdf/1510.00149.pdf)
[`code`](https://github.com/songhan/Deep-Compression-AlexNet)

- __[2015-NIPS] BinaryConnect: Training Deep Neural Networks with binary weights during propagations__ [`paper`](http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf)
[`code`](https://github.com/MatthieuCourbariaux/BinaryConnect)

- __[2016-NIPS] Dynamic Network Surgery for Efficient DNNs__[`paper`](http://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns.pdf)

- __[2016-NIPS] Learning Structured Sparsity in Deep Neural Networks__[`paper`](http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf)

- __[2017-CVPR] A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning__[`paper`](a gift from knowledge distillation fast optimization network minimization and transfer learning)

- __[2016-ICLR] Neural Networks with Few Multiplications__ [`paper`](https://arxiv.org/pdf/1510.03009.pdf)
[`code`](https://github.com/hantek/BinaryConnect)

- __[2016-ArXiv] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1__ [`paper`](https://arxiv.org/pdf/1602.02830.pdf)
[`code`](https://github.com/MatthieuCourbariaux/BinaryNet)


- __[2016-ECCV] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks__ [`paper`](https://arxiv.org/pdf/1603.05279.pdf)
[`code`](https://github.com/allenai/XNOR-Net)

- __[2014-NIPS] Distilling the Knowledge in a Neural Network__ [`paper`](https://arxiv.org/pdf/1503.02531.pdf)
[`code`](https://github.com/allenai/XNOR-Net)

- __[2017-CVPR] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications__ [`paper`](https://arxiv.org/pdf/1704.04861.pdf)
[`code`](https://github.com/Zehaos/MobileNet)

- __[2018-CVPR] MobileNetV2: Inverted Residuals and Linear Bottlenecks__ [`paper`](https://arxiv.org/pdf/1801.04381.pdf)
[`code`](https://github.com/MG2033/MobileNet-V2)

- __[2017-CVPR] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices__ [`paper`](https://arxiv.org/pdf/1707.01083.pdf) [`code`](https://github.com/MG2033/ShuffleNet)

## Pruning
- __[2017-ICLR] Pruning Filters for Efficient ConvNets__ [`paper`](https://arxiv.org/pdf/1608.08710.pdf)
[`code`](https://github.com/slothkong/DNN-Pruning)

ðŸŒŸ
- __[2017-ICCV] Channel Pruning for Accelerating Very Deep Neural Networks__ [`paper`](https://arxiv.org/pdf/1707.06168.pdf)
[`code`](https://github.com/yihui-he/channel-pruning)

- __[2017-ICCV] ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression__ [`paper`](https://arxiv.org/pdf/1707.06168.pdf)[`code`](https://github.com/Roll920/ThiNet)
[`code`](https://github.com/MG2033/MobileNet-V2)






